{
  "name": "dnn",
  "tagline": "A light-weight yet efficient framework for deep learning",
  "body": "# Deep Neural Nets\r\n\r\n[![Build Status](https://travis-ci.org/liangfu/dnn.svg?branch=master)](https://travis-ci.org/liangfu/dnn)\r\n[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)\r\n\r\n## Introduction\r\n\r\nThe Deep Neural Nets (DNN) library is a deep learning framework designed to be small in size, computationally efficient and portable.\r\n\r\nWe started the project as a fork of the popular [OpenCV](http://opencv.org/) library,\r\nwhile removing some components that is not tightly related to the deep learning framework.\r\nComparing to Caffe and many other implements, DNN is relatively independent to third-party libraries, \r\n(Yes, we don't require Boost and Database systems to be install before crafting your own network models)\r\nand it can be more easily portable to mobile systems, like iOS, Android and RaspberryPi etc.\r\n\r\n## Available Modules\r\n\r\nThe following features have been implemented:\r\n\r\n - Mini-batch based learning, with OpenMP support\r\n - YAML based network definition\r\n - Gradient checking for all implemented layers\r\n\r\nThe following modules are implemented in current version:\r\n\r\n Module Name           | Description\r\n ---                   | ---\r\n `InputDataLayer`      | Data Container Layer, for storing original input images\r\n `ConvolutionLayer`    | Convolutional Neural Network Layer, performs 2d convolution upon images\r\n `SubSamplingLayer`    | Sub-Sampling Layer, performs max-pooling operation\r\n `FullConnectLayer`    | Fully Connected Layer, with activation options, e.g. tanh, sigmoid, softmax, relu etc.\r\n `RecurrentNNLayer`    | vallina Recurrent Neural Network (RNN) Layer, for processing sequence data\r\n `CombineLayer`        | Combine Layer, for combining output results from multiple different layers\r\n\r\nMore modules will be available online !\r\n\r\n### Network Definition\r\n\r\nLayer Type | Attributes\r\n--- | ---\r\n`InputData` | `name`,`n_input_planes`,`input_height`,`input_width`,`seq_length`\r\n`Convolution` | `name`,`visualize`,`n_output_planes`,`ksize`\r\n`SubSampling` | `name`,`visualize`,`ksize`\r\n`FullConnect` | `name`,`input_layer(optional)`,`visualize`,`n_output_planes`,`activation_type`\r\n`RecurrentNN` | `name`,`n_output_planes`,`seq_length`,`time_index`,`activation_type`\r\n`Combine` | `name`,`input_layers`,`visualize`,`n_output_planes`\r\n\r\nWith the above parameters given in YAML format, one can simply define a network. \r\nFor instance, a modifed lenet can be:\r\n\r\n```yaml\r\n%YAML:1.0\r\nlayers:\r\n  - {type: InputData, name: input1, n_input_planes: 1, input_height: 28, input_width: 28, seq_length: 1}\r\n  - {type: Convolution, name: conv1, visualize: 0, n_output_planes: 6, ksize: 5, stride: 1}\r\n  - {type: SubSampling, name: pool1, visualize: 0, ksize: 2, stride: 2}\r\n  - {type: Convolution, name: conv2, visualize: 0, n_output_planes: 16, ksize: 5, stride: 1}\r\n  - {type: SubSampling, name: pool2, visualize: 0, ksize: 2, stride: 2}\r\n  - {type: FullConnect, name: fc1, visualize: 0, n_output_planes: 10, activation_type: tanh}\r\n```\r\n\r\nThen, by ruuning network training program:\r\n\r\n```bash\r\n$ network train --solver data/mnist/lenet_solver.xml\r\n```\r\n\r\none can start to train a simple network right away. And this is the way the source code and data models are tested in Travis-Ci. (See [.travis.yml](https://github.com/liangfu/dnn/blob/master/.travis.yml) in the root directory)\r\n\r\n## Compilation\r\n\r\n[CMake](https://cmake.org) is required for successfully compiling the project. \r\n\r\nUnder root directory of the project:\r\n\r\n ```bash\r\n $ cd $DNN_ROOT\r\n $ mkdir build\r\n $ cmake .. \r\n $ make -j4\r\n ```\r\n\r\n## License\r\n\r\nMIT\r\n\r\n<script>\r\n  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){\r\n  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),\r\n  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)\r\n  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');\r\n  ga('create', 'UA-8286931-2', 'auto');\r\n  ga('send', 'pageview');\r\n</script>\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}