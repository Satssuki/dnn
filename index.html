<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>dnn by liangfu</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>dnn</h1>
        <p>A light-weight yet efficient framework for deep learning</p>

        <p class="view"><a href="https://github.com/liangfu/dnn">View the Project on GitHub <small>liangfu/dnn</small></a></p>


        <ul>
          <li><a href="https://github.com/liangfu/dnn/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/liangfu/dnn/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/liangfu/dnn">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h1>
<a id="deep-neural-nets" class="anchor" href="#deep-neural-nets" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Deep Neural Nets</h1>

<p><a href="https://travis-ci.org/liangfu/dnn"><img src="https://travis-ci.org/liangfu/dnn.svg?branch=master" alt="Build Status"></a>
<a href="LICENSE"><img src="https://img.shields.io/badge/license-MIT-blue.svg" alt="License"></a></p>

<h2>
<a id="introduction" class="anchor" href="#introduction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Introduction</h2>

<p>The Deep Neural Nets (DNN) library is a deep learning framework designed to be small in size, computationally efficient and portable.</p>

<p>We started the project as a fork of the popular <a href="http://opencv.org/">OpenCV</a> library,
while removing some components that is not tightly related to the deep learning framework.
Comparing to Caffe and many other implements, DNN is relatively independent to third-party libraries, 
(Yes, we don't require Boost and Database systems to be install before crafting your own network models)
and it can be more easily portable to mobile systems, like iOS, Android and RaspberryPi etc.</p>

<h2>
<a id="available-modules" class="anchor" href="#available-modules" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Available Modules</h2>

<p>The following features have been implemented:</p>

<ul>
<li>Mini-batch based learning, with OpenMP support</li>
<li>YAML based network definition</li>
<li>Gradient checking for all implemented layers</li>
</ul>

<p>The following modules are implemented in current version:</p>

<table>
<thead>
<tr>
<th>Module Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>InputDataLayer</code></td>
<td>Data Container Layer, for storing original input images</td>
</tr>
<tr>
<td><code>ConvolutionLayer</code></td>
<td>Convolutional Neural Network Layer, performs 2d convolution upon images</td>
</tr>
<tr>
<td><code>SubSamplingLayer</code></td>
<td>Sub-Sampling Layer, performs max-pooling operation</td>
</tr>
<tr>
<td><code>FullConnectLayer</code></td>
<td>Fully Connected Layer, with activation options, e.g. tanh, sigmoid, softmax, relu etc.</td>
</tr>
<tr>
<td><code>RecurrentNNLayer</code></td>
<td>vallina Recurrent Neural Network (RNN) Layer, for processing sequence data</td>
</tr>
<tr>
<td><code>CombineLayer</code></td>
<td>Combine Layer, for combining output results from multiple different layers</td>
</tr>
</tbody>
</table>

<p>More modules will be available online !</p>

<h3>
<a id="network-definition" class="anchor" href="#network-definition" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Network Definition</h3>

<table>
<thead>
<tr>
<th>Layer Type</th>
<th>Attributes</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>InputData</code></td>
<td>
<code>name</code>,<code>n_input_planes</code>,<code>input_height</code>,<code>input_width</code>,<code>seq_length</code>
</td>
</tr>
<tr>
<td><code>Convolution</code></td>
<td>
<code>name</code>,<code>visualize</code>,<code>n_output_planes</code>,<code>ksize</code>
</td>
</tr>
<tr>
<td><code>SubSampling</code></td>
<td>
<code>name</code>,<code>visualize</code>,<code>ksize</code>
</td>
</tr>
<tr>
<td><code>FullConnect</code></td>
<td>
<code>name</code>,<code>input_layer(optional)</code>,<code>visualize</code>,<code>n_output_planes</code>,<code>activation_type</code>
</td>
</tr>
<tr>
<td><code>RecurrentNN</code></td>
<td>
<code>name</code>,<code>n_output_planes</code>,<code>seq_length</code>,<code>time_index</code>,<code>activation_type</code>
</td>
</tr>
<tr>
<td><code>Combine</code></td>
<td>
<code>name</code>,<code>input_layers</code>,<code>visualize</code>,<code>n_output_planes</code>
</td>
</tr>
</tbody>
</table>

<p>With the above parameters given in YAML format, one can simply define a network. 
For instance, a modifed lenet can be:</p>

<div class="highlight highlight-source-yaml"><pre>%<span class="pl-ent">YAML</span>:1.0
<span class="pl-s"><span class="pl-ent">layers:</span></span>
  <span class="pl-s">- <span class="pl-ent">{type:</span> <span class="pl-s">InputData, name: input1, n_input_planes: 1, input_height: 28, input_width: 28, seq_length: 1}</span></span>
  <span class="pl-s">- <span class="pl-ent">{type:</span> <span class="pl-s">Convolution, name: conv1, visualize: 0, n_output_planes: 6, ksize: 5, stride: 1}</span></span>
  <span class="pl-s">- <span class="pl-ent">{type:</span> <span class="pl-s">SubSampling, name: pool1, visualize: 0, ksize: 2, stride: 2}</span></span>
  <span class="pl-s">- <span class="pl-ent">{type:</span> <span class="pl-s">Convolution, name: conv2, visualize: 0, n_output_planes: 16, ksize: 5, stride: 1}</span></span>
  <span class="pl-s">- <span class="pl-ent">{type:</span> <span class="pl-s">SubSampling, name: pool2, visualize: 0, ksize: 2, stride: 2}</span></span>
  <span class="pl-s">- <span class="pl-ent">{type:</span> <span class="pl-s">FullConnect, name: fc1, visualize: 0, n_output_planes: 10, activation_type: tanh}</span></span></pre></div>

<p>Then, by ruuning network training program:</p>

<div class="highlight highlight-source-shell"><pre>$ network train --solver data/mnist/lenet_solver.xml</pre></div>

<p>one can start to train a simple network right away. And this is the way the source code and data models are tested in Travis-Ci. (See <a href="https://github.com/liangfu/dnn/blob/master/.travis.yml">.travis.yml</a> in the root directory)</p>

<h2>
<a id="compilation" class="anchor" href="#compilation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Compilation</h2>

<p><a href="https://cmake.org">CMake</a> is required for successfully compiling the project. </p>

<p>Under root directory of the project:</p>

<div class="highlight highlight-source-shell"><pre> $ <span class="pl-c1">cd</span> <span class="pl-smi">$DNN_ROOT</span>
 $ mkdir build
 $ cmake .. 
 $ make -j4</pre></div>



<h2>
<a id="license" class="anchor" href="#license" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>License</h2>

<p>MIT</p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/liangfu">liangfu</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>
